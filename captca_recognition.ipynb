{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "59598779-2d51-4df2-a684-18afef8d23c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed lib\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7c40b50b-c486-41b9-8d31-a1595b8c4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the captcha \n",
    "def load_data(file_path):\n",
    "    n_samples = len(os.listdir(file_path))\n",
    "    imgs = []\n",
    "    labels = []\n",
    "\n",
    "    # read from the directory\n",
    "    for i, img in enumerate(os.listdir(file_path)):\n",
    "        if (i == 10):\n",
    "            break;\n",
    "        imgs.append(cv2.resize(cv2.imread(os.path.join(file_path, img), 0), (300, 57)))\n",
    "        labels.append(img[0:-6])\n",
    "\n",
    "    return np.array(imgs), labels\n",
    "\n",
    "    # # print the ddtype\n",
    "    # print(X[0].dtype) # uint8\n",
    "\n",
    "    # # preprocess the image\n",
    "    # for i in range(len(X)):\n",
    "    #     X[i] = cv2.resize(X[i], (300, 57)) # reshape it to the same size\n",
    "    #     X[i] = cv2.equalizeHist(X[i]) # increase the contrast of the img\n",
    "    #     # X[i] = cv2.medianBlur(X[i], (2)) # try to remove noise term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb62c173-2847-4fd8-b92f-941c3e23aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(imgs):\n",
    "    processed_data = imgs\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "efbeb9f6-29b2-4a39-b774-591a573181bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels as sequences of integers\n",
    "def encode_labels(labels, max_length):\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"))  # All possible characters\n",
    "    encoded_labels = [label_encoder.transform(list(label)) for label in labels]\n",
    "    padded_labels = np.array([np.pad(label, (0, max_length - len(label)), mode='constant') for label in encoded_labels]) # ? do we need to pad\n",
    "    return padded_labels, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aa92f95e-17ee-4f2e-804e-ba3c6f263344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Reshape, LSTM, Dense, Lambda\n",
    "import keras.backend as K\n",
    "\n",
    "# Define the CNN + RNN model\n",
    "def create_crnn_model(input_shape, num_classes, max_length=6):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # CNN layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Reshape for RNN\n",
    "    x = Reshape((259, 128))(x)  # Reshape to (time_steps, features)\n",
    "\n",
    "    # RNN layers\n",
    "    x = LSTM(128, return_sequences=True)(x)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(num_classes + 1, activation='softmax')(x)  # +1 for CTC blank token\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4709a016-08ed-480f-9ddd-c01e54e8f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC loss function\n",
    "import tensorflow as tf\n",
    "\n",
    "def ctc_loss(y_true, y_pred):\n",
    "    # Get batch size, input length, and label length\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "    input_length = tf.shape(y_pred)[1]\n",
    "    label_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Cast input_length and label_length to int64\n",
    "    input_length = tf.cast(input_length, dtype=tf.int64)\n",
    "    label_length = tf.cast(label_length, dtype=tf.int64)\n",
    "\n",
    "    # Create tensors for input_length and label_length\n",
    "    input_length = input_length * tf.ones((batch_size, 1), dtype=tf.int64)\n",
    "    label_length = label_length * tf.ones((batch_size, 1), dtype=tf.int64)\n",
    "\n",
    "    # Compute CTC loss\n",
    "    return tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "de2c9975-bcc0-4f9c-96cd-e32a9693969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2cc92a07-ef4d-454b-8e3d-d93b032d68d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a CAPTCHA image\n",
    "def predict_captcha(image_path):\n",
    "    # Preprocess the image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (300, 57))  # Resize to match model input\n",
    "    image = image / 255.0  # Normalize\n",
    "    image = image.reshape(1, 57, 300, 1)  # Reshape for model input\n",
    "\n",
    "    # Predict the CAPTCHA\n",
    "    predictions = model.predict(image)\n",
    "    decoded = K.ctc_decode(predictions, input_length=np.ones(predictions.shape[0]) * predictions.shape[1], greedy=True)[0][0]\n",
    "    decoded = K.eval(decoded)  # Convert tensor to numpy array\n",
    "    captcha_text = \"\".join(label_encoder.inverse_transform(decoded[0]))\n",
    "    return captcha_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3455bb09-24b5-4fc4-9db0-7e99dbacf2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    correct = 0\n",
    "    for i in range(len(X_val)):\n",
    "        image = X_val[i].reshape(1, 32, 128, 1)\n",
    "        prediction = model.predict(image)\n",
    "        decoded = K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0]) * prediction.shape[1], greedy=True)[0][0]\n",
    "        decoded = K.eval(decoded)\n",
    "        predicted_text = \"\".join(label_encoder.inverse_transform(decoded[0]))\n",
    "        true_text = \"\".join(label_encoder.inverse_transform(y_val[i]))\n",
    "        if predicted_text == true_text:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(X_val)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = evaluate_model(model, X_val, y_val)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a651a079-536f-4153-a80e-0816e9188ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 988.0058 - val_loss: 955.2221\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452ms/step - loss: 949.3893 - val_loss: 994.0943\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 501ms/step - loss: 910.9783 - val_loss: 908.1239\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473ms/step - loss: 858.3888 - val_loss: 928.1896\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 558ms/step - loss: 833.3911 - val_loss: 894.3937\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 687ms/step - loss: 787.4226 - val_loss: 850.4167\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 541ms/step - loss: 737.7437 - val_loss: 831.0126\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step - loss: 704.5043 - val_loss: 808.0816\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 550ms/step - loss: 665.7467 - val_loss: 797.8944\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 672ms/step - loss: 637.3680 - val_loss: 788.0859\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447ms/step - loss: 610.6055 - val_loss: 784.7255\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444ms/step - loss: 586.7290 - val_loss: 785.6003\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 565.9360 - val_loss: 785.9434\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step - loss: 545.8407 - val_loss: 787.6348\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477ms/step - loss: 527.0232 - val_loss: 789.1208\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 587ms/step - loss: 510.5266 - val_loss: 792.0503\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487ms/step - loss: 495.6056 - val_loss: 794.3552\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - loss: 482.9464 - val_loss: 799.5148\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433ms/step - loss: 471.4118 - val_loss: 805.0898\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 529ms/step - loss: 461.1056 - val_loss: 812.6767\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # loading data\n",
    "    imgs, labels = load_data(\"./data/train\") # (10, 57, 300), (10)\n",
    "\n",
    "    # # print the ddtype\n",
    "    # print(imgs[0].dtype) # uint8\n",
    "    \n",
    "    # # print the original img\n",
    "    # plt.figure(figsize=(8,8))\n",
    "    # for i in range(len(imgs)):\n",
    "    #     plt.subplot(5, 2, i+1)\n",
    "    #     plt.imshow(imgs[i])\n",
    "    #     plt.xlabel(f\"{labels[i]}\")\n",
    "    # plt.show()\n",
    "\n",
    "    # preprocess data\n",
    "    processed_imgs = preprocess_data(imgs)\n",
    "\n",
    "    # encode labels\n",
    "    max_length = max(len(label) for label in labels)\n",
    "    encoded_labels, label_encoder = encode_labels(labels, max_length)\n",
    "    # print(label_encoder.inverse_transform(encoded_labels[0]), labels[0]) # check for the correctness of the encoding\n",
    "\n",
    "    # Reshape images for CNN input\n",
    "    processed_imgs_reshape = processed_imgs.reshape(-1, 57, 300, 1)  # (batch_size, height, width, channels)\n",
    "\n",
    "    # Create the model\n",
    "    input_shape = (57, 300, 1)  # Input image shape (height, width, channels)\n",
    "    num_classes = len(label_encoder.classes_)  # Number of unique characters\n",
    "    model = create_crnn_model(input_shape, num_classes)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=ctc_loss)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_val, y_train, y_val = split_data(processed_imgs, encoded_labels)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(\"captcha_crnn_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
